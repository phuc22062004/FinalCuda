# Section 3: Ph√¢n T√≠ch Hi·ªáu NƒÉng To√†n Di·ªán (Comprehensive Performance Analysis)

## 3.1 So S√°nh Hi·ªáu NƒÉng Qua C√°c Giai ƒêo·∫°n (Performance Comparison Across All Phases)

### B·∫£ng T·ªïng H·ª£p Hi·ªáu NƒÉng (Complete Performance Summary Table)

| Phase | Implementation | Training Time (1K imgs, 3 epochs) | Time/Epoch | Speedup (vs CPU) | Incremental Speedup | Memory Usage (batch=64) | Key Optimization |
|-------|---------------|-----------------------------------|------------|------------------|---------------------|------------------------|------------------|
| **2.1** | **CPU Baseline** | 2,250s | 750s | 1.0√ó | - | ~200 MB (RAM) | Sequential execution |
| **2.2** | **GPU Basic** | 9.53s | 3.18s | **236√ó** ‚≠ê | 236√ó | 441 MiB (VRAM) | Naive parallelization |
| **2.3** | **GPU Opt V1** | 6.18s | 2.06s | **364√ó** üèÜ | 1.54√ó | 617 MiB (VRAM) | Shared memory tiling + coalescing |
| **2.4** | **GPU Opt V2** | 8.25s | 2.75s | **273√ó** | 0.75√ó (slower) | 437 MiB (VRAM) | Kernel fusion + vectorization |

**Note on V2 Performance:** Phase 2.4 (GPU Opt V2) demonstrates that not all optimizations lead to better performance. While kernel fusion reduced memory usage by 29%, the removal of shared memory tiling from V1 caused a 33% slowdown. This is a valuable lesson in understanding optimization trade-offs.

### Full Training Performance (50K images, 3 epochs, batch=64)

| Phase | Total Time | Time/Epoch | Throughput | Speedup vs CPU | Memory | Status |
|-------|-----------|------------|------------|----------------|--------|--------|
| **CPU Baseline** | ~62,500s (17.4 hrs) | ~20,833s | ~2.4 imgs/sec | 1.0√ó | ~200 MB | ‚ùå Too slow |
| **GPU Basic** | ~317s (5.3 min) | ~106s | ~472 imgs/sec | ~197√ó | 441 MiB | ‚úÖ Usable |
| **GPU Opt V1** | **231s (3.9 min)** | **77s** | **649 imgs/sec** | **270√ó** üèÜ | 617 MiB | ‚úÖ‚úÖ Best |
| **GPU Opt V2** | 335s (5.6 min) | 112s | 448 imgs/sec | ~187√ó | 437 MiB | ‚úÖ Acceptable |

**Performance Ranking:** V1 (231s) > Basic (317s) > V2 (335s) >> CPU (62,500s)

### SVM Integration Performance (Phase 2.5)

| Operation | Dataset | Time | Throughput | Details |
|-----------|---------|------|------------|---------|
| **Feature Extraction (GPU)** | 50K train | 19s | 2,632 imgs/sec | Encoder forward pass only |
| **Feature Extraction (GPU)** | 10K test | 5s | 2,000 imgs/sec | Same encoder weights |
| **Total GPU Extraction** | 60K images | 24s | 2,500 imgs/sec | Pure computation |
| **Z-Score Scaling** | 50K train | 183s | 273 imgs/sec | 2-pass: compute stats + scale |
| **LibSVM File Writing** | 50K train | 183s | 273 imgs/sec | Text format bottleneck |
| **Total Feature Pipeline** | 60K images | 247s | 243 imgs/sec | Extraction + I/O |
| **SVM Training (cuML GPU)** | 50K samples | 65.83s | 759 samples/sec | RBF kernel, C=10 |
| **SVM Prediction (cuML GPU)** | 10K samples | 21.32s | 469 samples/sec | GPU-accelerated |
| **End-to-End Classification** | Test set | - | **65.57% accuracy** | 6,557/10,000 correct |

### Memory Usage Analysis

| Phase | CPU/GPU | Memory Type | Peak Usage | Notes |
|-------|---------|-------------|------------|-------|
| **CPU Baseline** | CPU | System RAM | ~200 MB | Host memory for weights + activations |
| **GPU Basic** | GPU | VRAM | 441 MiB | Naive allocation, no optimization |
| **GPU Opt V1** | GPU | VRAM | **617 MiB (+40%)** | Shared memory tiles + batch buffers |
| **GPU Opt V2** | GPU | VRAM | 437 MiB (-29% vs V1) | Kernel fusion eliminates intermediate buffers |
| **SVM Model** | GPU | VRAM + Disk | 13.5 GB (saved) | Support vectors for 50K samples |

**Memory Insights:**
- V1 uses more memory due to shared memory tiles and optimization buffers
- V2 reduces memory by fusing operations (fewer intermediate buffers)
- Trade-off: V1's extra memory ‚Üí 33% faster execution

### Ph√¢n T√≠ch ƒêi·ªÉm Ngh·∫Ωn Qua C√°c Giai ƒêo·∫°n (Bottleneck Analysis Across Phases)

#### Phase 2.2: GPU Basic
```
Kernel Time Distribution:
  Conv kernels:        98.0% (4.83s out of 4.93s)
  ReLU:                0.5%
  Pooling/Upsample:    1.0%
  Loss computation:    0.5%

API Time Distribution:
  cudaMemcpy:          73.0% (3.16s out of 4.33s)
  cudaLaunchKernel:    24.7%
  Other:               2.3%

Primary Bottleneck: Global memory bandwidth (no data reuse)
```

#### Phase 2.3: GPU Opt V1
```
Kernel Time Distribution:
  Conv forward:         9.8% (0.44s out of 4.50s) ‚Üê 11√ó faster than Basic!
  Conv backward:       87.6% (3.94s)
  Other:                2.6%

API Time Distribution:
  cudaMemcpy:          90.9% (4.33s out of 4.76s)
  cudaLaunchKernel:     6.6%
  Other:                2.5%

Primary Bottleneck: Backward pass + memory transfers
Optimization Success: Shared memory tiling ‚Üí 11√ó forward pass speedup
```

#### Phase 2.4: GPU Opt V2
```
Kernel Time Distribution:
  Conv forward:        23.4% (1.53s out of 6.54s) ‚Üê 3.5√ó SLOWER than V1!
  Conv backward:       74.8% (4.89s)
  Loss (vec4):          0.2% (14.9ms)
  SGD (vec4):           0.0% (1.2ms)

API Time Distribution:
  cudaMemcpy:          92.3% (6.30s out of 6.82s)
  cudaLaunchKernel:     6.0%
  Other:                1.7%

Primary Bottleneck: Lost shared memory optimization
Regression Root Cause: Kernel fusion without tiling ‚Üí slower than V1
```

### Ti·∫øn Tr√¨nh TƒÉng Hi·ªáu NƒÉng T√≠ch L≈©y (Cumulative Performance Gains)

```
Progress Timeline:

CPU Baseline (750s/epoch)
    ‚Üì [+236√ó speedup - GPU parallelization]
GPU Basic (3.18s/epoch)
    ‚Üì [+1.54√ó speedup - Shared memory]
GPU Opt V1 (2.06s/epoch) ‚Üê BEST PERFORMANCE ‚≠ê
    ‚Üì [0.75√ó regression - Kernel fusion trade-off]
GPU Opt V2 (2.75s/epoch)
```

**Total Speedup Achieved:** CPU ‚Üí GPU V1 = **364√ó faster**

### Visualization Requirements

#### 1. Training Time Comparison (Bar Chart)

**Recommended Visualization:**
```python
import matplotlib.pyplot as plt
import numpy as np

phases = ['CPU\nBaseline', 'GPU\nBasic', 'GPU\nOpt V1', 'GPU\nOpt V2']
times = [750, 3.18, 2.06, 2.75]  # seconds per epoch (1K images)
colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']

plt.figure(figsize=(12, 6))
bars = plt.bar(phases, times, color=colors, edgecolor='black', linewidth=1.5)

# Add value labels
for bar, time in zip(bars, times):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{time:.2f}s', ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.ylabel('Time per Epoch (seconds, log scale)', fontsize=12)
plt.yscale('log')
plt.title('Training Time Comparison Across Optimization Phases\n(1,000 images, 3 epochs)', 
          fontsize=14, fontweight='bold')
plt.grid(axis='y', alpha=0.3, linestyle='--')
plt.tight_layout()
plt.savefig('training_time_comparison.png', dpi=300)
```

**Chart Description:**
- X-axis: 4 implementation phases
- Y-axis: Time/epoch (log scale to show dramatic differences)
- Shows dramatic drop from CPU (750s) to GPU Basic (3.18s)
- Highlights best performance (GPU V1: 2.06s)
- Shows V2 regression (2.75s)

#### 2. Cumulative Speedup (Line Graph)

**Recommended Visualization:**
```python
import matplotlib.pyplot as plt

phases = ['CPU\nBaseline', 'GPU\nBasic', 'GPU\nOpt V1', 'GPU\nOpt V2']
speedups = [1.0, 236, 364, 273]  # vs CPU baseline

plt.figure(figsize=(12, 6))
plt.plot(phases, speedups, marker='o', linewidth=3, markersize=12, 
         color='#2ca02c', markerfacecolor='#ff7f0e', markeredgewidth=2, 
         markeredgecolor='#2ca02c')

# Annotate peak
plt.annotate('Peak: 364√ó', xy=(2, 364), xytext=(2.3, 320),
            arrowprops=dict(arrowstyle='->', lw=2, color='red'),
            fontsize=12, fontweight='bold', color='red')

# Annotate regression
plt.annotate('Regression:\n-25%', xy=(3, 273), xytext=(2.5, 240),
            arrowprops=dict(arrowstyle='->', lw=2, color='orange'),
            fontsize=11, fontweight='bold', color='orange')

plt.ylabel('Speedup Factor (vs CPU Baseline)', fontsize=12)
plt.title('Cumulative Speedup Across Optimization Phases', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, linestyle='--')
plt.ylim(0, 400)
plt.axhline(y=236, color='gray', linestyle='--', alpha=0.5, label='GPU Basic baseline')
plt.legend(fontsize=10)
plt.tight_layout()
plt.savefig('cumulative_speedup.png', dpi=300)
```

**Chart Description:**
- Shows progression of speedup: 1√ó ‚Üí 236√ó ‚Üí 364√ó ‚Üí 273√ó
- Highlights peak performance at V1 (364√ó)
- Clearly shows V2 regression
- Illustrates optimization journey

#### 3. Memory vs Performance Trade-off (Scatter Plot)

**Recommended Visualization:**
```python
import matplotlib.pyplot as plt

phases = ['GPU\nBasic', 'GPU\nOpt V1', 'GPU\nOpt V2']
memory = [441, 617, 437]  # MiB
time = [3.18, 2.06, 2.75]  # seconds/epoch
colors = ['#ff7f0e', '#2ca02c', '#1f77b4']
sizes = [200, 300, 200]  # marker sizes

plt.figure(figsize=(10, 6))
for i, (phase, mem, t, color, size) in enumerate(zip(phases, memory, time, colors, sizes)):
    plt.scatter(mem, t, s=size, c=color, alpha=0.7, edgecolors='black', linewidth=2)
    plt.annotate(phase, (mem, t), xytext=(10, -10), textcoords='offset points',
                fontsize=11, fontweight='bold')

plt.xlabel('GPU Memory Usage (MiB)', fontsize=12)
plt.ylabel('Time per Epoch (seconds)', fontsize=12)
plt.title('Memory vs Performance Trade-off', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# Add ideal region annotation
plt.axhline(y=2.5, color='green', linestyle='--', alpha=0.3, label='Target: <2.5s')
plt.axvline(x=500, color='blue', linestyle='--', alpha=0.3, label='Target: <500 MiB')
plt.legend(fontsize=10)
plt.tight_layout()
plt.savefig('memory_vs_performance.png', dpi=300)
```

**Chart Description:**
- X-axis: Memory usage (MiB)
- Y-axis: Time per epoch (seconds)
- Best position: bottom-left (fast + low memory)
- V1: Fast but high memory (617 MiB)
- V2: Low memory but slower (437 MiB)
- Shows trade-offs visually

#### 4. SVM Classification Results (Confusion Matrix)

Already generated in Phase 2.5: `confusion_matrix_cuml.png`

**Key Features:**
- 10√ó10 heatmap for CIFAR-10 classes
- Strong diagonal (correct predictions)
- Cat-Dog confusion visible
- Ship class best performance (77.2%)
- Bird class worst performance (50.1%)

---

## 3.2 Ph√¢n T√≠ch T√°c ƒê·ªông C·ªßa C√°c T·ªëi ∆Øu (Optimization Impact Analysis)

### Nh·ªØng G√¨ Ho·∫°t ƒê·ªông T·ªët (What Worked Well)

| Optimization | Phase | Impact | Evidence |
|-------------|-------|--------|----------|
| **GPU Parallelization** | 2.2 | +236√ó speedup | CPU 750s ‚Üí GPU 3.18s |
| **Shared Memory Tiling** | 2.3 | +1.54√ó speedup | Conv forward: 11√ó faster (4.83s ‚Üí 0.44s) |
| **Memory Coalescing** | 2.3 | Bandwidth efficiency | threadIdx.x for width dimension |
| **In-place ReLU** | 2.3 | Memory reduction | Reuse conv output buffers |
| **Gradient Buffer Reuse** | 2.3 | Memory reduction | Reuse d_grad_up2, d_grad_relu4 |
| **Vectorized SGD (float4)** | 2.4 | Minimal impact | ~1ms saved (0.01% of total) |
| **Vectorized Loss (float4)** | 2.4 | Minimal impact | ~5ms saved (0.06% of total) |

### Nh·ªØng G√¨ Kh√¥ng Ho·∫°t ƒê·ªông (What Didn't Work)

| Optimization | Phase | Impact | Reason |
|-------------|-------|--------|--------|
| **Kernel Fusion** | 2.4 | -33% regression | Lost shared memory tiling |
| **No Shared Memory** | 2.4 | 3.5√ó slower forward | Global memory bandwidth bottleneck |
| **Loop Unrolling** | 2.4 | Negligible | Compiler already optimizes 3√ó3 loops |

### Key Insight: Optimization Priority Matters

**Optimization Impact Hierarchy:**
1. **Memory Access Patterns** (shared memory): **11√ó impact** ‚≠ê‚≠ê‚≠ê
2. **Parallelization Strategy**: **236√ó impact** ‚≠ê‚≠ê‚≠ê
3. **Memory Coalescing**: **~1.5√ó impact** ‚≠ê‚≠ê
4. **Kernel Fusion**: **Negative impact without (1)** ‚ùå
5. **Vectorization (float4)**: **<0.1% impact** ‚≠ê

**Lesson:** Optimize memory access first, then computation, then minor details.

---

# Section 4: B√†i H·ªçc R√∫t Ra V√† Th√°ch Th·ª©c ƒê√£ V∆∞·ª£t Qua

## 4.1 Nh·ªØng Hi·ªÉu Bi·∫øt K·ªπ Thu·∫≠t Quan Tr·ªçng

### A. L·∫≠p Tr√¨nh CUDA

**1. Memory Hierarchy L√† Quan Tr·ªçng Nh·∫•t**
- Shared memory (on-chip) nhanh h∆°n global memory 100√ó
- Coalesced access t·ªët h∆°n strided access 10√ó v·ªÅ bƒÉng th√¥ng
- Phase 2.3: shared memory tiling ‚Üí tƒÉng 11√ó t·ªëc ƒë·ªô forward pass
- Phase 2.4: m·∫•t shared memory ‚Üí ch·∫≠m 3.5√ó

```cuda
// T·ªêT: C√°c threads h·ª£p t√°c load tile m·ªôt l·∫ßn, t·∫•t c·∫£ t√°i s·ª≠ d·ª•ng
__shared__ float s_tile[TILE_HEIGHT][TILE_WIDTH];
// Load tile cooperatively
__syncthreads();
for (int kh = 0; kh < 3; kh++) {
    for (int kw = 0; kw < 3; kw++) {
        sum += s_tile[...] * weight[...];  // Nhanh!
    }
}
```

**2. T·ªï Ch·ª©c Thread ƒê√∫ng C√°ch**
- threadIdx.x n√™n map v·ªõi memory li√™n ti·∫øp (coalescing)
- Block size 16√ó16 (256 threads) c√¢n b·∫±ng t·ªët
- Grid dimensions kh·ªõp v·ªõi output dimensions

**3. Profiling L√† B·∫Øt Bu·ªôc**
- Gi·∫£ ƒë·ªãnh v·ªÅ hi·ªáu nƒÉng th∆∞·ªùng sai
- nsys (Nsight Systems) thi·∫øt y·∫øu cho t·ªëi ∆∞u CUDA
- V√≠ d·ª• d·ª± √°n: gi·∫£ ƒë·ªãnh kernel fusion t·ªët h∆°n ‚Üí th·ª±c t·∫ø ch·∫≠m 33%
- **B√†i h·ªçc**: Lu√¥n ƒëo l∆∞·ªùng, kh√¥ng ƒëo√°n m√≤

**4. Kh√¥ng Ph·∫£i T·ªëi ∆Øu N√†o C≈©ng K·∫øt H·ª£p ƒê∆∞·ª£c**
- Lo·∫°i b·ªè m·ªôt t·ªëi ∆∞u c√≥ th·ªÉ ph√° v·ª° t·ªëi ∆∞u kh√°c
- Phase 2.4: kernel fusion lo·∫°i b·ªè intermediate writes (t·ªët) nh∆∞ng c≈©ng lo·∫°i b·ªè shared memory tiling (x·∫•u)
- **C√°ch ƒë√∫ng**: Gi·ªØ t·ªëi ∆∞u c≈©, TH√äM t·ªëi ∆∞u m·ªõi l√™n tr√™n, kh√¥ng thay th·∫ø

### B. H·ªçc S√¢u (Deep Learning)

**1. Ch·∫•t L∆∞·ª£ng Features C·ªßa Autoencoder**
- M·ª•c ti√™u reconstruction ‚â† m·ª•c ti√™u classification
- Unsupervised features v·∫´n ƒë·∫°t 65% accuracy (t·ªët!)
- Cat/Dog confusion do reconstruction t∆∞∆°ng t·ª±
- Bird kh√≥ nh·∫•t (50.1%) - v·∫≠t th·ªÉ nh·ªè, ƒë·ªô ph√¢n gi·∫£i h·∫°n ch·∫ø

**2. Trade-offs C·ªßa Two-Stage Pipeline**
- **∆Øu ƒëi·ªÉm**: Nhanh (11 ph√∫t), d·ªÖ hi·ªÉu, modular
- **Nh∆∞·ª£c ƒëi·ªÉm**: ƒê·ªô ch√≠nh x√°c k√©m h∆°n supervised CNN 20%
- **So s√°nh**: 65.57% accuracy vs 85-90% supervised, nh∆∞ng nhanh 20√ó

**3. T√°c ƒê·ªông C·ªßa Batch Size**
- GPU h∆∞·ªüng l·ª£i t·ª´ batch l·ªõn h∆°n (32 ‚Üí 64)
- CPU gi·ªõi h·∫°n b·ªüi memory (batch=32 max)
- GPU batch=64: 2.06s/epoch, memory 441 MiB

### C. T·ªëi ∆Øu Hi·ªáu NƒÉng

**1. ƒê·ªãnh Lu·∫≠t Amdahl Trong Th·ª±c T·∫ø**
- D√π tƒÉng 364√ó, v·∫´n c√≤n bottlenecks
- Feature extraction (GPU): 24s
- LibSVM I/O: 183s (273 imgs/sec)
- **Bottleneck chuy·ªÉn**: Computation ‚Üí I/O

$$Speedup = \frac{1}{(1-P) + \frac{P}{S}}$$

**2. T·ªëi ∆Øu C√≥ Diminishing Returns**
- T·ªëi ∆∞u 1: +236√ó (parallelization)
- T·ªëi ∆∞u 2: +1.54√ó (shared memory)
- T·ªëi ∆∞u 3: -0.25√ó (regression)
- **Quy lu·∫≠t**: L·ª£i √≠ch l·ªõn nh·∫•t ƒë·∫øn tr∆∞·ªõc, bi·∫øt l√∫c d·ª´ng!

**3. Metrics S·ª≠ D·ª•ng Ph·∫ßn C·ª©ng**
- GPU Utilization: 99% (xu·∫•t s·∫Øc)
- Memory Bandwidth: ~70% of peak (t·ªët)
- SM Occupancy: ~80% (ch·∫•p nh·∫≠n ƒë∆∞·ª£c)
- Power: 127W / 400W (32% - compute-bound)

---

## 4.2 Nh·ªØng Th√°ch Th·ª©c L·ªõn V√† Gi·∫£i Ph√°p

### Th√°ch Th·ª©c 1: B·ªë Tr√≠ B·ªô Nh·ªõ V√† Coalescing

**V·∫•n ƒë·ªÅ:** Tri·ªÉn khai GPU ban ƒë·∫ßu c√≥ memory bandwidth th·∫•p do non-coalesced access. Profiling ch·ªâ ƒë·∫°t 30% peak bandwidth.

**Gi·∫£i ph√°p:** S·∫Øp x·∫øp l·∫°i thread-to-data mapping sao cho threadIdx.x t∆∞∆°ng ·ª©ng v·ªõi width dimension (W), ƒë·∫£m b·∫£o threads trong warp truy c·∫≠p ƒë·ªãa ch·ªâ li√™n ti·∫øp.

**B√†i h·ªçc:** Memory access patterns quan tr·ªçng h∆°n computation. Kernel coalesced ƒë∆°n gi·∫£n c√≥ th·ªÉ nhanh h∆°n kernel ph·ª©c t·∫°p kh√¥ng coalesced.

### Th√°ch Th·ª©c 2: Xung ƒê·ªôt Bank Trong Shared Memory

**V·∫•n ƒë·ªÅ:** Khi tri·ªÉn khai shared memory tiling ·ªü Phase 2.3, phi√™n b·∫£n ƒë·∫ßu c√≥ xung ƒë·ªôt bank nghi√™m tr·ªçng (8-way conflicts). Speedup ch·ªâ 2√ó thay v√¨ 10√ó.

**Gi·∫£i ph√°p:** Th√™m padding v√†o shared memory tile. ƒê·ªïi t·ª´ `__shared__ float s_tile[16][16]` sang `__shared__ float s_tile[16][18]` (th√™m 2 c·ªôt). ƒê·∫£m b·∫£o threads kh√°c nhau truy c·∫≠p banks kh√°c nhau.

**B√†i h·ªçc:** Shared memory c·∫ßn thi·∫øt k·∫ø layout c·∫©n th·∫≠n ƒë·ªÉ tr√°nh bank conflicts. Thay ƒë·ªïi nh·ªè trong dimensions c√≥ th·ªÉ c·∫£i thi·ªán hi·ªáu nƒÉng ƒë√°ng k·ªÉ.

### Th√°ch Th·ª©c 3: Debug GPU Kernel

**V·∫•n ƒë·ªÅ:** Sau khi port convolution l√™n GPU, output ho√†n to√†n sai (NaN, Inf, random values). Kh√≥ debug v√¨ kh√¥ng c√≥ stack traces. M·∫•t 2 ng√†y t√¨m bug.

**Gi·∫£i ph√°p:** Chi·∫øn l∆∞·ª£c debug c√≥ h·ªá th·ªëng:
1. Gi·∫£m k√≠ch th∆∞·ªõc xu·ªëng 1 ·∫£nh, 1 channel, spatial dimensions nh·ªè
2. Verify output tr√™n CPU v·ªõi c√πng input nh·ªè
3. D√πng `cudaMemcpy` copy activations v·ªÅ host ƒë·ªÉ so s√°nh
4. Th√™m `assert()` cho boundary checks (debug mode)
5. T√¨m th·∫•y bug: t√≠nh index sai cho padding trong convolution kernel

**B√†i h·ªçc:** GPU debugging c·∫ßn chi·∫øn l∆∞·ª£c kh√°c CPU. Lu√¥n verify correctness tr√™n input nh·ªè tr∆∞·ªõc khi scale up. D√πng `cudaDeviceSynchronize()` v√† `cudaGetLastError()` sau m·ªói kernel launch khi develop.

### Th√°ch Th·ª©c 4: Phase 2.4 Regression - Kernel Fusion Ph·∫£n T√°c D·ª•ng

**V·∫•n ƒë·ªÅ:** Tri·ªÉn khai kernel fusion ƒë·ªÉ k·∫øt h·ª£p Conv+Bias+ReLU th√†nh 1 kernel (Phase 2.4), mong ƒë·ª£i tƒÉng 20-30% nh∆∞ng l·∫°i ch·∫≠m 33%. Profiling cho th·∫•y conv forward ch·∫≠m 3.5√ó so v·ªõi Phase 2.3.

**Gi·∫£i ph√°p:** Ph√¢n t√≠ch profiling data v√† nh·∫≠n ra kernel fusion ƒë√£ lo·∫°i b·ªè shared memory tiling t·ª´ Phase 2.3. Fused kernel ƒë·ªçc tr·ª±c ti·∫øp t·ª´ global memory thay v√¨ t√°i s·ª≠ d·ª•ng t·ª´ shared memory tiles. Revert v·ªÅ Phase 2.3 v√† document trade-off analysis.

**B√†i h·ªçc:** Kh√¥ng ph·∫£i t·ªëi ∆∞u n√†o c≈©ng c·∫£i thi·ªán hi·ªáu nƒÉng. M·ªôt s·ªë t·ªëi ∆∞u conflict v·ªõi nhau. Lu√¥n profile tr∆∞·ªõc v√† sau m·ªói thay ƒë·ªïi. S·∫µn s√†ng revert t·ªëi ∆∞u kh√¥ng th√†nh c√¥ng. Document failures nh∆∞ b√†i h·ªçc qu√Ω gi√°.

### Th√°ch Th·ª©c 5: Ngh·∫Ωn I/O LibSVM

**V·∫•n ƒë·ªÅ:** ƒê·∫°t 24 gi√¢y GPU feature extraction, nh∆∞ng to√†n b·ªô SVM pipeline m·∫•t 415 gi√¢y. Profiling cho th·∫•y 71% th·ªùi gian (296s) load LibSVM text files, kh√¥ng ph·∫£i SVM training.

**Gi·∫£i ph√°p:**
- X√°c ƒë·ªãnh LibSVM text format l√† bottleneck
- Chuy·ªÉn sang binary caching: l∆∞u features d·∫°ng binary
- D√πng cuML GPU-accelerated SVM (65s training vs 300s+ CPU LibSVM)
- ƒê·ªÅ xu·∫•t t∆∞∆°ng lai: HDF5 ho·∫∑c NPZ thay v√¨ LibSVM text

**B√†i h·ªçc:** T·ªëi ∆∞u compute v√¥ d·ª•ng n·∫øu I/O l√† bottleneck. T·ªëi ∆∞u end-to-end pipeline quan tr·ªçng h∆°n ch·ªâ t·ªëi ∆∞u kernels ƒë∆°n l·∫ª. Text formats ch·∫≠m; d√πng binary formats cho datasets l·ªõn.

---

## 4.3 C√°c K·ªπ NƒÉng ƒê√£ N·∫Øm V·ªØng

### L·∫≠p Tr√¨nh CUDA
- ‚úÖ Thi·∫øt k·∫ø kernel v√† launch configuration
- ‚úÖ Qu·∫£n l√Ω memory hierarchy (global, shared, constant)
- ‚úÖ T·ªï ch·ª©c threads v√† coalescing
- ‚úÖ Shared memory tiling v·ªõi padding
- ‚úÖ Atomic operations cho reductions
- ‚úÖ Error checking v√† debugging strategies
- ‚úÖ Profiling v·ªõi Nsight Systems (nsys)

### Deep Learning
- ‚úÖ Thi·∫øt k·∫ø ki·∫øn tr√∫c autoencoder
- ‚úÖ Forward v√† backward propagation
- ‚úÖ Loss functions (MSE cho reconstruction)
- ‚úÖ Thu·∫≠t to√°n t·ªëi ∆∞u (SGD)
- ‚úÖ Tr√≠ch xu·∫•t features cho transfer learning
- ‚úÖ Two-stage pipeline (unsupervised + supervised)

### T·ªëi ∆Øu Hi·ªáu NƒÉng
- ‚úÖ T·ªëi ∆∞u d·ª±a tr√™n profiling
- ‚úÖ X√°c ƒë·ªãnh v√† ph√¢n t√≠ch bottlenecks
- ‚úÖ T·ªëi ∆∞u memory bandwidth
- ‚úÖ Ph√¢n t√≠ch trade-offs (t·ªëc ƒë·ªô vs b·ªô nh·ªõ)
- ‚úÖ Hi·ªÉu diminishing returns
- ‚úÖ Bi·∫øt khi n√†o n√™n d·ª´ng t·ªëi ∆∞u

---

# Section 5: K·∫øt Lu·∫≠n V√† H∆∞·ªõng Ph√°t Tri·ªÉn T∆∞∆°ng Lai (Conclusion and Future Work)

## 5.1 T√≥m T·∫Øt D·ª± √Ån (Project Summary)

### Nh·ªØng G√¨ ƒê√£ Ho√†n Th√†nh (What Was Accomplished)

We successfully implemented and optimized a complete two-stage pipeline for unsupervised feature learning and image classification on CIFAR-10:

**Stage 1: Autoencoder Training (GPU Optimized)**
- Implemented CNN-based autoencoder from scratch in CUDA
- 5 convolutional layers + 2 pooling layers (encoder)
- 5 convolutional layers + 2 upsampling layers (decoder)
- Trained on 50,000 CIFAR-10 images (3 channels, 32√ó32 pixels)
- Achieved reconstruction loss convergence in 3 epochs

**Stage 2: SVM Classification**
- Extracted 8,192-dimensional features from encoder bottleneck
- Trained RBF-kernel SVM on learned features (cuML GPU)
- Achieved 65.57% classification accuracy on CIFAR-10 test set
- Per-class analysis revealing model strengths and weaknesses

**Optimization Journey:**
- **Phase 2.1**: CPU baseline implementation (750s/epoch)
- **Phase 2.2**: Naive GPU parallelization ‚Üí 236√ó speedup
- **Phase 2.3**: Shared memory + coalescing ‚Üí 364√ó speedup ‚≠ê
- **Phase 2.4**: Kernel fusion attempt ‚Üí regression analysis
- **Phase 2.5**: Complete pipeline with SVM integration

### Final Performance Metrics Summary

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| **Autoencoder Training Time** | < 10 min | **3.9 min** (231s, 50K imgs) | ‚úÖ **Exceeded** |
| **Feature Extraction Time** | < 20 sec | **24 sec** (60K imgs, GPU only) | ‚ö†Ô∏è Close (I/O: 247s) |
| **Classification Accuracy** | 60-65% | **65.57%** | ‚úÖ **Met** |
| **GPU Speedup vs CPU** | > 20√ó | **364√ó** | ‚úÖ **Far exceeded** |
| **End-to-End Pipeline** | N/A | **~11 min total** | ‚úÖ Production-ready |

### Achievement of Original Objectives

**Primary Objectives:**
1. ‚úÖ **Implement autoencoder architecture** - Complete with all layers
2. ‚úÖ **Train on CIFAR-10** - 50K images, unsupervised learning
3. ‚úÖ **Optimize with CUDA** - 364√ó speedup achieved
4. ‚úÖ **Extract meaningful features** - 65.57% classification proves quality
5. ‚úÖ **Integrate with SVM** - Complete two-stage pipeline working

**Learning Objectives:**
1. ‚úÖ **Master CUDA programming** - Kernels, memory, profiling
2. ‚úÖ **Understand deep learning** - Forward/backward propagation
3. ‚úÖ **Apply optimization techniques** - Shared memory, coalescing, tiling
4. ‚úÖ **Analyze performance** - Profiling, bottlenecks, trade-offs
5. ‚úÖ **Document findings** - Comprehensive reports for all phases

---

## 5.2 Nh·ªØng Th√†nh T·ª±u Ch√≠nh (Key Achievements)

### üèÜ TƒÉng T·ªëc T·ªëi ƒêa: 364√ó (CPU ‚Üí GPU Opt V1)

**Details:**
- CPU Baseline: 750 seconds/epoch (1,000 images)
- GPU Opt V1: 2.06 seconds/epoch (1,000 images)
- **Speedup: 750 / 2.06 = 364√ó**

**Breakdown:**
- Parallelization (Phase 2.2): 236√ó speedup
- Shared memory tiling (Phase 2.3): 1.54√ó additional ‚Üí 364√ó cumulative
- Full training (50K images): 231 seconds (3.9 minutes)

**Impact:**
- CPU training: 17.4 hours (impractical)
- GPU training: 3.9 minutes (production-ready)
- Enables rapid experimentation and iteration

### üìä Classification Accuracy: 65.57%

**Details:**
- 6,557 correct predictions out of 10,000 test images
- Unsupervised features (no labels during autoencoder training)
- RBF-kernel SVM with C=10, gamma=scale

**Per-Class Performance:**
- Best: Ship (77.2%), Automobile (74.1%), Frog (72.4%)
- Worst: Bird (50.1%), Cat (55.1%), Dog (55.8%)
- Variance: 27.1% gap between best and worst

**Comparison:**
- Random guess: 10%
- Raw pixels + SVM: ~40%
- HOG features + SVM: ~45%
- **Our approach: 65.57%** ‚úÖ
- Supervised CNN: 85-90% (upper bound)

**Interpretation:**
- 65.57% is excellent for unsupervised features
- 20% gap to supervised CNN is expected (reconstruction vs classification objective)
- Features learned without labels prove to be discriminative

### ‚ö° Most Successful Optimization: Shared Memory Tiling (Phase 2.3)

**Impact:**
- Convolution forward pass: **11√ó faster** (4.83s ‚Üí 0.44s)
- Overall training: **1.54√ó faster** (9.53s ‚Üí 6.18s)
- Memory bandwidth: 30% ‚Üí 70% of peak utilization

**Implementation:**
```cuda
// Key insight: Tile input, reuse across threads
__shared__ float s_input[TILE_SIZE + 2][TILE_SIZE + 2];  // +2 for padding

// Threads collaborate to load tile
// Each thread loads 1 element
s_input[ty][tx] = global_input[...];
__syncthreads();

// All threads compute using shared tile (no more global reads!)
for (kh, kw) {
    sum += s_input[ty + kh][tx + kw] * weight[...];
}
```

**Why It Worked:**
- Data reuse: Each input pixel used by multiple output pixels
- Reduced global memory accesses: 9√ó reduction (3√ó3 kernel)
- On-chip memory: 100√ó faster than global memory
- Coalesced loading: Threads load consecutive addresses

**Lesson:**
This single optimization provided more benefit than all other Phase 2.4 optimizations combined. Demonstrates the importance of understanding hardware architecture and memory hierarchy.

### üéì Technical Skills Mastered

**CUDA Programming (Advanced Level):**
- ‚úÖ Kernel design for 2D convolution, pooling, upsampling
- ‚úÖ Shared memory management with bank conflict avoidance
- ‚úÖ Memory coalescing for bandwidth optimization
- ‚úÖ Atomic operations for parallel reductions
- ‚úÖ Profiling with Nsight Systems (nsys)
- ‚úÖ Debugging GPU kernels with systematic strategies

**Deep Learning Implementation:**
- ‚úÖ CNN architecture design from scratch
- ‚úÖ Forward propagation with multiple layer types
- ‚úÖ Backward propagation and gradient computation
- ‚úÖ SGD optimization algorithm
- ‚úÖ Loss function implementation (MSE)
- ‚úÖ Feature extraction and transfer learning

**Performance Engineering:**
- ‚úÖ Bottleneck identification through profiling
- ‚úÖ Optimization trade-off analysis (speed vs memory)
- ‚úÖ Understanding Amdahl's Law in practice
- ‚úÖ Knowing when to stop optimizing
- ‚úÖ End-to-end pipeline thinking (not just kernels)

**Machine Learning Pipeline:**
- ‚úÖ Two-stage learning (unsupervised + supervised)
- ‚úÖ Feature scaling (Z-score normalization)
- ‚úÖ SVM integration with GPU acceleration (cuML)
- ‚úÖ Model evaluation and confusion matrix analysis
- ‚úÖ Understanding model limitations

---

## 5.3 H·∫°n Ch·∫ø

### C√°c ƒêi·ªÉm Ngh·∫Ωn Hi·ªáu NƒÉng

**1. BƒÉng Th√¥ng B·ªô Nh·ªõ (Gi·ªõi h·∫°n ph·∫ßn c·ª©ng)**
- Phase 2.3 ƒë·∫°t ~70% bƒÉng th√¥ng t·ªëi ƒëa c·ªßa A100 (1,089/1,555 GB/s)
- cudaMemcpy chi·∫øm 90.9% API time, backward pass chi·∫øm 87.6% kernel time
- Kh√≥ t·ªëi ∆∞u th√™m do ƒë√£ s·ª≠ d·ª•ng shared memory v√† memory coalescing

**2. Ngh·∫Ωn I/O (Ngo√†i CUDA)**
- LibSVM text format: 296s ƒë·ªÉ load 60K samples (71% th·ªùi gian SVM)
- GPU extraction ch·ªâ 24s, nh∆∞ng to√†n b·ªô pipeline 247s do I/O
- Gi·∫£i ph√°p: s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng binary (HDF5, NPZ) thay v√¨ text

**3. Backward Pass Chi Ph·ªëi**
- Forward pass t·ªëi ∆∞u (0.44s), backward pass chi·∫øm 87.6% (3.94s)
- Gradient computation b·ªã gi·ªõi h·∫°n b·ªüi memory bandwidth
- Ch∆∞a √°p d·ª•ng: Mixed precision (FP16), gradient checkpointing

### H·∫°n Ch·∫ø ƒê·ªô Ch√≠nh X√°c

**1. M·ª•c Ti√™u Unsupervised Kh√¥ng Ph√π H·ª£p**
- Autoencoder hu·∫•n luy·ªán cho reconstruction, kh√¥ng ph·∫£i classification
- ƒê·ªô ch√≠nh x√°c 65.57% t·ªët cho unsupervised, nh∆∞ng k√©m h∆°n supervised CNN 20%
- Cat/Dog confusion do t·∫≠p trung v√†o reconstruction, kh√¥ng ph·∫£i ph√¢n bi·ªát class

**2. ƒê·ªô Ph√¢n Gi·∫£i Bottleneck Th·∫•p**
- Bottleneck 8√ó8 c√≥ th·ªÉ qu√° th√¥ cho chi ti·∫øt nh·ªè
- ·∫¢nh h∆∞·ªüng ƒë·∫øn v·∫≠t th·ªÉ nh·ªè (Bird ch·ªâ 50.1% accuracy)
- TƒÉng l√™n 16√ó16 s·∫Ω tƒÉng 4√ó s·ªë features nh∆∞ng t·ªën b·ªô nh·ªõ v√† th·ªùi gian SVM

**3. Ch·ªçn Kernel SVM Ch∆∞a T·ªëi ∆Øu**
- RBF kernel v·ªõi gamma auto-tune, C=10.0 ch∆∞a qua grid search
- Ch∆∞a th·ª≠ linear kernel ho·∫∑c ensemble methods
- ∆Øu ti√™n t·ªëi ∆∞u CUDA h∆°n l√† tuning SVM

### R√†ng Bu·ªôc Tri·ªÉn Khai

**1. Single-GPU**
- To√†n b·ªô code ch·∫°y tr√™n 1 GPU, kh√¥ng h·ªó tr·ª£ multi-GPU
- Kh√¥ng overlap H2D transfer v·ªõi compute
- H·∫°n ch·∫ø scalability

**2. Ch·ªâ FP32**
- Ch∆∞a √°p d·ª•ng mixed precision (FP16)
- M·∫•t c∆° h·ªôi tƒÉng t·ªëc 2-4√ó t·ª´ Tensor Cores
- L√Ω do: ƒë·ªô ph·ª©c t·∫°p v√† th·ªùi gian testing

**3. Batch Size Gi·ªõi H·∫°n**
- T·ªëi ƒëa batch=64 do activations cho backward pass
- Ch·ªâ d√πng 8% VRAM c·ªßa A100 (617 MiB / 40 GB)
- C√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng gradient checkpointing

---

## 5.4 H∆∞·ªõng C·∫£i Ti·∫øn T∆∞∆°ng Lai

### Ng·∫Øn H·∫°n (1-2 tu·∫ßn)

**1. CUDA Streams - Th·ª±c thi b·∫•t ƒë·ªìng b·ªô**
- Overlap transfer H2D/D2H v·ªõi kernel execution
- K·ª≥ v·ªçng: tƒÉng 10-15% t·ªëc ƒë·ªô end-to-end

**2. T·ªëi ∆∞u I/O SVM - Format binary**
- Chuy·ªÉn t·ª´ LibSVM text sang HDF5/NPZ
- K·ª≥ v·ªçng: gi·∫£m data loading t·ª´ 296s ‚Üí 10s (30√ó)

**3. Gradient Checkpointing**
- ƒê√°nh ƒë·ªïi compute ƒë·ªÉ gi·∫£m memory
- K·ª≥ v·ªçng: tƒÉng batch size 64 ‚Üí 128, tƒÉng 1.2-1.3√ó t·ªëc ƒë·ªô

### Trung H·∫°n (1-2 th√°ng)

**4. Mixed Precision Training (FP16)**
- S·ª≠ d·ª•ng Tensor Cores c·ªßa A100
- K·ª≥ v·ªçng: tƒÉng 2√ó t·ªëc ƒë·ªô, gi·∫£m 2√ó memory

**5. Contrastive Learning (SimCLR/MoCo)**
- Thay reconstruction b·∫±ng discriminative objective
- K·ª≥ v·ªçng: tƒÉng accuracy t·ª´ 65.57% ‚Üí 75-80%

**6. Kernel Fusion + Shared Memory**
- K·∫øt h·ª£p ∆∞u ƒëi·ªÉm Phase 2.3 v√† 2.4
- K·ª≥ v·ªçng: tƒÉng 1.2-1.5√ó so v·ªõi V1

### D√†i H·∫°n (3-6 th√°ng)

**7. Multi-GPU Data Parallelism**
- S·ª≠ d·ª•ng NCCL ƒë·ªÉ ƒë·ªìng b·ªô gradients
- K·ª≥ v·ªçng: g·∫ßn nh∆∞ linear speedup (2 GPUs ‚Üí 1.9√ó)

**8. Mega-Kernel Fusion**
- Fuse to√†n b·ªô encoder th√†nh 1 kernel
- K·ª≥ v·ªçng: gi·∫£m kernel launches, tƒÉng 2-3√ó t·ªëc ƒë·ªô

**9. Im2col + cuBLAS GEMM**
- Chuy·ªÉn convolution th√†nh matrix multiplication
- K·ª≥ v·ªçng: ƒë·∫°t 90-95% peak performance (hi·ªán t·∫°i 70%)

**10. End-to-End Supervised Fine-Tuning**
- B·ªè decoder, th√™m classification head
- K·ª≥ v·ªçng: accuracy 65.57% ‚Üí 85-90%

---

## 5.5 K·∫øt Lu·∫≠n

### Nh·ªØng ƒêi·ªÅu ƒê√£ Ch·ª©ng Minh

D·ª± √°n th√†nh c√¥ng ch·ª©ng minh:
1. **TƒÉng t·ªëc GPU** ƒë·∫°t 364√ó so v·ªõi CPU
2. **Unsupervised learning** t·∫°o features h·ªØu √≠ch (65.57% accuracy)
3. **Hi·ªÉu b·∫£n ch·∫•t t·ªëi ∆∞u quan tr·ªçng h∆°n √°p d·ª•ng m√π qu√°ng**: Shared memory > kernel fusion
4. **Kh√¥ng ph·∫£i t·ªëi ∆∞u n√†o c≈©ng hi·ªáu qu·∫£**: Phase 2.4 regression l√† b√†i h·ªçc qu√Ω gi√°
5. **T∆∞ duy end-to-end**: I/O tr·ªü th√†nh bottleneck sau khi t·ªëi ∆∞u GPU

### B√†i H·ªçc Quan Tr·ªçng Nh·∫•t

T·ªëi ∆∞u hi·ªáu nƒÉng kh√¥ng ph·∫£i l√† √°p d·ª•ng m·ªçi k·ªπ thu·∫≠t, m√† l√†:
- **Profiling** ƒë·ªÉ t√¨m bottleneck th·∫≠t s·ª± (kh√¥ng ph·∫£i ƒëo√°n)
- **Hi·ªÉu** t·∫°i sao t·ªëi ∆∞u ho·∫°t ƒë·ªông (memory hierarchy, data reuse)
- **ƒêo l∆∞·ªùng** tr∆∞·ªõc v√† sau m·ªói thay ƒë·ªïi
- **Ch·∫•p nh·∫≠n** khi t·ªëi ∆∞u th·∫•t b·∫°i (Phase 2.4)
- **Bi·∫øt l√∫c d·ª´ng** (diminishing returns)

### Ph√°t Tri·ªÉn C√° Nh√¢n

- **Tr∆∞·ªõc**: Ki·∫øn th·ª©c CUDA l√Ω thuy·∫øt t·ª´ gi·∫£ng ƒë∆∞·ªùng
- **Sau**: Kinh nghi·ªám th·ª±c t·∫ø t·ªëi ∆∞u deep learning workload
- **K·ªπ nƒÉng**: Profiling, debugging, ph√¢n t√≠ch trade-off
- **T·ª± tin**: C√≥ th·ªÉ tackle c√°c d·ª± √°n GPU computing trong t∆∞∆°ng lai

### ·ª®ng D·ª•ng T∆∞∆°ng Lai

- √Åp d·ª•ng CUDA optimization cho c√°c m√¥ h√¨nh kh√°c (ResNet, Transformer)
- M·ªü r·ªông ra datasets l·ªõn h∆°n (ImageNet, 1M+ images)
- Kh√°m ph√° multi-GPU v√† distributed training
- Tri·ªÉn khai c√°c k·ªπ thu·∫≠t state-of-the-art (mixed precision, advanced fusion)

##K·∫øt Th√∫c B√°o C√°o**

---

## Ph·ª• L·ª•c: B·∫£ng Tham Kh·∫£o Nhanh

### Hi·ªáu NƒÉng T·ªïng Quan

| Phi√™n b·∫£n | Th·ªùi gian (1K, 3 epochs) | TƒÉng t·ªëc | B·ªô nh·ªõ | Tr·∫°ng th√°i |
|---------|---------------------|---------|--------|--------|
| CPU Baseline | 2,250s | 1√ó | 200 MB | Ch·∫≠m |
| GPU Basic | 9.53s | 236√ó | 441 MiB | T·ªët |
| **GPU Opt V1** | **6.18s** | **364√ó** | 617 MiB | **T·ªët nh·∫•t** ‚≠ê |
| GPU Opt V2 | 8.25s | 273√ó | 437 MiB | Regression |

### K·∫øt Qu·∫£ Classification

| Ch·ªâ s·ªë | Gi√° tr·ªã |
|--------|-------|
| **ƒê·ªô ch√≠nh x√°c t·ªïng** | **65.57%** |
| Class t·ªët nh·∫•t | Ship (77.2%) |
| Class kh√≥ nh·∫•t | Bird (50.1%) |
| Ch√™nh l·ªách | 27.1% |
| Precision (trung b√¨nh) | 66% |
| Recall (trung b√¨nh) | 66% |
| F1-Score (trung b√¨nh) | 66% |

### S·ª≠ D·ª•ng Ph·∫ßn C·ª©ng

| T√†i nguy√™n | M·ª©c s·ª≠ d·ª•ng | ƒê√°nh gi√° |
|----------|-------------|--------|
| GPU Compute | 99% | ‚úÖ Xu·∫•t s·∫Øc |
| Memory Bandwidth | ~70% of peak | ‚úÖ T·ªët |
| C√¥ng su·∫•t | 127W / 400W (32%) | ‚ö†Ô∏è C√≥ th·ªÉ cao h∆°n |
| SM Occupancy | ~80% | ‚úÖ T·ªët |

### T√°c ƒê·ªông C√°c T·ªëi ∆Øu

| T·ªëi ∆∞u | T√°c ƒë·ªông | ƒê√°nh gi√° |
|-------------|--------|-----------|
| GPU Parallelization | +236√ó | ‚úÖ‚úÖ‚úÖ C·∫ßn thi·∫øt |
| Shared Memory Tiling | +1.54√ó | ‚úÖ‚úÖ‚úÖ C·∫ßn thi·∫øt |
| Memory Coalescing | +1.2√ó | ‚úÖ‚úÖ R·∫•t t·ªët |
| Kernel Fusion (no tiling) | -25% | ‚ùå Kh√¥ng ƒë√°ng |
| Vectorization (float4) | <0.1% | ‚ö†Ô∏è √çt hi·ªáu qu·∫£
|-------------|--------|-----------|
| GPU Parallelization | +236√ó | ‚úÖ‚úÖ‚úÖ Essential |
| Shared Memory Tiling | +1.54√ó | ‚úÖ‚úÖ‚úÖ Essential |
| Memory Coalescing | +1.2√ó | ‚úÖ‚úÖ Very good |
| Kernel Fusion (no tiling) | -25% | ‚ùå Not worth |
| Vectorization (float4) | <0.1% | ‚ö†Ô∏è Marginal |
