# HÆ¯á»šNG DáºªN CHáº Y Äáº¦Y Äá»¦ - CUDA AUTOENCODER + SVM PIPELINE

> **CSC14120 Final Project - Complete Step-by-Step Guide**  
> Tá»« build â†’ train â†’ extract features â†’ SVM classification

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng quan](#1-tá»•ng-quan)
2. [Cáº¥u trÃºc project](#2-cáº¥u-trÃºc-project)
3. [Chuáº©n bá»‹ mÃ´i trÆ°á»ng](#3-chuáº©n-bá»‹-mÃ´i-trÆ°á»ng)
4. [Build táº¥t cáº£ phiÃªn báº£n](#4-build-táº¥t-cáº£-phiÃªn-báº£n)
5. [Train Autoencoder](#5-train-autoencoder)
6. [Extract Features vá»›i Scaling](#6-extract-features-vá»›i-scaling)
7. [Train SVM](#7-train-svm)
8. [So sÃ¡nh káº¿t quáº£](#8-so-sÃ¡nh-káº¿t-quáº£)
9. [Troubleshooting](#9-troubleshooting)

---

## 1. Tá»”NG QUAN

Pipeline gá»“m 2 giai Ä‘oáº¡n:

```
[CIFAR-10 Images]
      â†“
[Stage 1: Train Autoencoder] â†’ weights.bin
      â†“
[Stage 2: Extract Features] â†’ features.libsvm (with Z-score scaling)
      â†“
[Stage 3: Train SVM] â†’ svm_model
      â†“
[Predict + Accuracy]
```

### CÃ¡c phiÃªn báº£n Autoencoder

| PhiÃªn báº£n | MÃ´ táº£ | Thá»i gian (200 epochs) | Optimization |
|-----------|-------|------------------------|--------------|
| **CPU** | Baseline C++ | ~40-50 phÃºt | KhÃ´ng cÃ³ GPU |
| **CUDA Basic** | Naive CUDA | ~8-10 phÃºt | Basic parallelization |
| **CUDA OPT_V1** | Memory optimized | ~6-7 phÃºt | Coalescing + Constant memory |
| **CUDA OPT_V2** | Speed optimized | ~4-5 phÃºt | Kernel fusion + Vectorization |

---

## 2. Cáº¤U TRÃšC PROJECT

```
FinalCuda/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main_cpu.cpp              # CPU entry point
â”‚   â”œâ”€â”€ main_cuda.cpp             # CUDA entry point
â”‚   â”œâ”€â”€ cpu/
â”‚   â”‚   â””â”€â”€ autoencoder_cpu.cpp   # CPU implementation
â”‚   â”œâ”€â”€ cuda/
â”‚   â”‚   â”œâ”€â”€ autoencoder_basic.cu  # CUDA basic
â”‚   â”‚   â”œâ”€â”€ autoencoder_opt_v1.cu # Memory optimized
â”‚   â”‚   â””â”€â”€ autoencoder_opt_v2.cu # Speed optimized
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ cifar10_loader.h      # CIFAR-10 loader
â”‚   â””â”€â”€ svm/
â”‚       â””â”€â”€ extract_features_cuda.cpp  # Feature extraction + Z-score scaling
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ autoencoder.hpp           # CPU header
â”‚   â”œâ”€â”€ autoencoder_cuda.h        # CUDA header
â”‚   â””â”€â”€ config.h                  # Hyperparameters
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_cpu.sh              # Build CPU
â”‚   â”œâ”€â”€ build_cuda.sh             # Build CUDA versions
â”‚   â””â”€â”€ build_svm.sh              # Build feature extractors
â”œâ”€â”€ cifar-10-binary/
â”‚   â””â”€â”€ cifar-10-batches-bin/     # Dataset
â”œâ”€â”€ build_cpu/                    # CPU executables
â”œâ”€â”€ build_cuda/                   # CUDA executables
â””â”€â”€ build_svm/                    # SVM tools
```

---

## 3. CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG

### 3.1. YÃªu cáº§u há»‡ thá»‘ng

- **OS**: Linux (Ubuntu 20.04+) hoáº·c WSL2
- **CUDA**: 11.0+ (tested with CUDA 12.0)
- **GPU**: NVIDIA GPU vá»›i compute capability 6.0+ (RTX 3050 hoáº·c cao hÆ¡n)
- **Compiler**: GCC 9+ vÃ  nvcc
- **RAM**: 8GB+ (16GB recommended)
- **Disk**: ~10GB free space

### 3.2. Kiá»ƒm tra CUDA

```bash
nvcc --version
nvidia-smi
```

### 3.3. Download CIFAR-10 dataset

Äáº£m báº£o folder `cifar-10-binary/cifar-10-batches-bin/` cÃ³ cÃ¡c file:
- `data_batch_1.bin` â†’ `data_batch_5.bin`
- `test_batch.bin`
- `batches.meta.txt`

---

## 4. BUILD Táº¤T Cáº¢ PHIÃŠN Báº¢N

### 4.1. Build CPU baseline

```bash
cd /home/senyamiku/LTSS/FinalCuda
chmod +x scripts/*.sh
./scripts/build_cpu.sh
```

**Output**: `build_cpu/autoencoder_cpu`

### 4.2. Build CUDA versions (Basic + OPT_V1 + OPT_V2)

```bash
./scripts/build_cuda.sh
```

**Output**:
- `build_cuda/autoencoder_cuda_basic`
- `build_cuda/autoencoder_cuda_opt_v1`
- `build_cuda/autoencoder_cuda_opt_v2`

### 4.3. Build SVM feature extractors

```bash
./scripts/build_svm.sh
```

**Output**:
- `build_svm/extract_features_cpu`
- `build_svm/extract_features_cuda` (basic)
- `build_svm/extract_features_cuda_opt_v1`

---

## 5. TRAIN AUTOENCODER

### 5.1. CÃº phÃ¡p lá»‡nh

```bash
./build_<version>/autoencoder_<version> \
    <cifar_dir> \
    <weights_output> \
    <num_epochs> \
    <batch_size> \
    <learning_rate> \
    [num_images]
```

### 5.2. Train CPU (kiá»ƒm tra baseline)

```bash
# Test vá»›i 1000 images, 10 epochs
./build_cpu/autoencoder_cpu \
    cifar-10-binary/cifar-10-batches-bin \
    weights_cpu_test.bin \
    10 32 0.001 1000
```

**Thá»i gian**: ~2-3 phÃºt cho 1000 images

### 5.3. Train CUDA Basic (full training)

```bash
# Full training: 50000 images, 200 epochs
./build_cuda/autoencoder_cuda_basic \
    cifar-10-binary/cifar-10-batches-bin \
    autoencoder_cuda_basic_weights.bin \
    200 32 0.001
```

**Thá»i gian**: ~8-10 phÃºt  
**Loss cuá»‘i**: ~0.02-0.03

### 5.4. Train CUDA OPT_V1 (memory optimized)

```bash
# Memory optimized vá»›i constant memory + coalescing
./build_cuda/autoencoder_cuda_opt_v1 \
    cifar-10-binary/cifar-10-batches-bin \
    autoencoder_cuda_opt_v1_weights.bin \
    200 32 0.001
```

**Thá»i gian**: ~6-7 phÃºt  
**Optimization**: 
- Memory coalescing (threadIdx.x cho width)
- Constant memory cho conv1/conv5 weights
- Gradient clipping [-1.0, 1.0]
- Removed redundant memset

### 5.5. Train CUDA OPT_V2 (speed optimized) âš¡

```bash
# Speed optimized vá»›i kernel fusion + vectorization
./build_cuda/autoencoder_cuda_opt_v2 \
    cifar-10-binary/cifar-10-batches-bin \
    autoencoder_cuda_opt_v2_weights.bin \
    200 32 0.001
```

**Thá»i gian**: ~4-5 phÃºt (~30% faster than OPT_V1)  
**Optimization**:
- Kernel fusion (conv+bias+relu in one kernel)
- Vectorized float4 for SGD updates
- Vectorized float4 for MSE loss
- Tuned block dimensions per layer
- Specialized hardcoded 3x3 kernels

**Test nhanh** (3 epochs, 1000 images):
```bash
time ./build_cuda/autoencoder_cuda_opt_v2 \
    cifar-10-binary/cifar-10-batches-bin \
    test_opt_v2.bin \
    3 32 0.001 1000
```

**Expected**: ~26 seconds for 3 epochs

---

## 6. EXTRACT FEATURES Vá»šI SCALING

### 6.1. Táº¡i sao cáº§n Z-score scaling?

Features tá»« autoencoder (ReLU outputs) cÃ³:
- PhÃ¢n phá»‘i khÃ´ng chuáº©n (toÃ n giÃ¡ trá»‹ dÆ°Æ¡ng)
- Variance khÃ´ng Ä‘á»“ng Ä‘á»u giá»¯a cÃ¡c chiá»u
- SVM RBF kernel hoáº¡t Ä‘á»™ng kÃ©m vá»›i unscaled features

**Káº¿t quáº£**:
- âŒ **KhÃ´ng scale**: ~46% accuracy
- âœ… **CÃ³ scale**: ~60-65% accuracy

### 6.2. Z-Score Scaling Pipeline (2-Pass)

```
PASS 1: Extract train â†’ Compute mean/std â†’ Cache to disk
        â†“ (finalize statistics)
        Save scaler_z.bin
PASS 2: Read cache â†’ Scale â†’ Write LibSVM
TEST:   Extract â†’ Scale (with loaded scaler) â†’ Write LibSVM
```

### 6.3. Extract vá»›i CUDA Basic

```bash
cd /home/senyamiku/LTSS/FinalCuda

# CÃº phÃ¡p: ./build_svm/extract_features_cuda <cifar_dir> <weights> [output_train] [output_test]
./build_svm/extract_features_cuda \
    cifar-10-binary/cifar-10-batches-bin \
    autoencoder_cuda_basic_weights.bin \
    train_features_cuda.libsvm \
    test_features_cuda.libsvm
```

**Output**:
- `train_features_cuda.libsvm` (~5.7GB, 50K samples, 8192 dims, scaled)
- `test_features_cuda.libsvm` (~1.2GB, 10K samples, scaled)
- `scaler_z.bin` (~97KB, mean/std statistics)
- `train_cache.bin` (~1.6GB, binary cache)

**Thá»i gian**: ~7-8 phÃºt cho 60K images

**Kiá»ƒm tra output**:
```bash
# Check format (should see negative and positive values)
head -2 train_features_cuda.libsvm
```

Expected:
```
6 1:-1.348 2:-0.584 3:-0.453 ...
3 11:-1.224 12:-0.826 ...
```

### 6.4. Extract vá»›i OPT_V1

```bash
./build_svm/extract_features_cuda_opt_v1 \
    cifar-10-binary/cifar-10-batches-bin \
    autoencoder_cuda_opt_v1_weights.bin \
    train_features_opt_v1.libsvm \
    test_features_opt_v1.libsvm
```

---

## 7. TRAIN SVM

### 7.1. CÃ i Ä‘áº·t ThunderSVM (recommended)

```bash
# Clone vÃ  build
git clone https://github.com/Xtra-Computing/thundersvm.git
cd thundersvm
mkdir build && cd build
cmake ..
make -j
```

Executables: `thundersvm-train`, `thundersvm-predict`

### 7.2. Train SVM vá»›i RBF kernel

```bash
# Default parameters (C=1, gamma=auto)
./thundersvm-train \
    -s 0 \
    -t 2 \
    -c 1.0 \
    -g 0.0001220703125 \
    train_features_cuda.libsvm \
    svm_model_cuda.txt
```

**Parameters**:
- `-s 0`: C-SVC classification
- `-t 2`: RBF kernel
- `-c 1.0`: Cost parameter
- `-g`: Gamma (1 / num_features = 1/8192 â‰ˆ 0.000122)

**Thá»i gian**: ~5-15 phÃºt tÃ¹y hardware

### 7.3. Grid search tá»‘i Æ°u (optional)

```bash
# TÃ¬m C vÃ  gamma tá»‘i Æ°u
for c in 0.1 1 10 100; do
  for g in 0.00001 0.0001 0.001; do
    echo "Testing C=$c, gamma=$g"
    ./thundersvm-train -s 0 -t 2 -c $c -g $g \
      train_features_cuda.libsvm model_c${c}_g${g}.txt
  done
done
```

### 7.4. Predict vÃ  Ä‘Ã¡nh giÃ¡

```bash
# Predict trÃªn test set
./thundersvm-predict \
    test_features_cuda.libsvm \
    svm_model_cuda.txt \
    predictions.txt

# Accuracy sáº½ Ä‘Æ°á»£c in ra console
```

**Expected accuracy**:
- Basic features (scaled): **~60-62%**
- OPT_V1 features (scaled): **~60-63%**
- Grid search optimized: **~63-65%**

---

## 8. SO SÃNH Káº¾T QUáº¢

### 8.1. Training time comparison

| Version | 200 epochs | 3 epochs (1K images) | Speedup vs CPU |
|---------|------------|----------------------|----------------|
| CPU | ~45 min | ~2.5 min | 1x |
| CUDA Basic | ~9 min | ~45s | 5x |
| CUDA OPT_V1 | ~6.5 min | ~36s | 7x |
| CUDA OPT_V2 | ~4.5 min | ~26s | 10x |

### 8.2. Feature extraction time

| Version | 60K images | Features per second |
|---------|------------|---------------------|
| CPU | ~15-20 min | ~50-60 img/s |
| CUDA Basic | ~7-8 min | ~120-140 img/s |
| CUDA OPT_V1 | ~6-7 min | ~140-160 img/s |

### 8.3. SVM accuracy (CIFAR-10)

| Features | Scaling | Accuracy |
|----------|---------|----------|
| Basic | âŒ No | ~46% |
| Basic | âœ… Z-score | **~60-62%** |
| OPT_V1 | âœ… Z-score | **~60-63%** |
| Grid search | âœ… Z-score | **~63-65%** |

### 8.4. Optimization summary

**OPT_V1** (Memory focused):
- âœ… Memory coalescing (threadIdx.x for width dimension)
- âœ… Constant memory for conv1/conv5 (54KB)
- âœ… Removed redundant cudaMemset (2ms saved)
- âœ… Gradient clipping to [-1.0, 1.0]
- âœ… `__restrict__` pointers + `#pragma unroll`

**OPT_V2** (Speed focused):
- âœ… All OPT_V1 optimizations
- âœ… Kernel fusion (conv+bias+relu â†’ 1 kernel)
- âœ… Vectorized float4 for SGD updates
- âœ… Vectorized float4 for MSE loss
- âœ… Tuned block dimensions (32Ã—8, 16Ã—16, 8Ã—8)
- âœ… Specialized hardcoded 3Ã—3 kernels
- âœ… `-O3 -use_fast_math` compiler flags

---

## 9. TROUBLESHOOTING

### 9.1. Gradient explosion

**Triá»‡u chá»©ng**: Loss tÄƒng Ä‘á»™t ngá»™t tá»« ~0.3 â†’ 50+

**NguyÃªn nhÃ¢n**: 
- Missing cudaMemset cho gradient buffers
- Gradient clipping quÃ¡ lá»ng

**Giáº£i phÃ¡p**:
```cpp
// Initialize gradient buffers
cudaMemset(d_grad_relu1, 0, size);
cudaMemset(d_grad_relu2, 0, size);

// Tighten gradient clipping
float val = fminf(fmaxf(grad, -1.0f), 1.0f);  // [-1.0, 1.0]
```

### 9.2. Out of memory

**Giáº£i phÃ¡p**:
- Giáº£m batch size: 32 â†’ 16
- Train vá»›i subset nhá» hÆ¡n
- Sá»­ dá»¥ng GPU cÃ³ VRAM lá»›n hÆ¡n

### 9.3. SVM accuracy tháº¥p (~46%)

**NguyÃªn nhÃ¢n**: Features khÃ´ng Ä‘Æ°á»£c scale

**Giáº£i phÃ¡p**: Äáº£m báº£o Ä‘Ã£ cháº¡y extract_features_cuda má»›i (cÃ³ Z-score scaling)

```bash
# Kiá»ƒm tra scaler_z.bin cÃ³ tá»“n táº¡i
ls -lh scaler_z.bin

# Kiá»ƒm tra features cÃ³ giÃ¡ trá»‹ Ã¢m (Ä‘Ã£ scale)
head -2 train_features_cuda.libsvm
```

### 9.4. Build errors

**nvcc not found**:
```bash
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

**Missing headers**:
```bash
# Kiá»ƒm tra include paths
ls include/
ls src/data/
```

---

## ğŸ“ QUICK START SCRIPT

```bash
#!/bin/bash
# Full pipeline from scratch

cd /home/senyamiku/LTSS/FinalCuda

# 1. Build táº¥t cáº£
./scripts/build_cuda.sh
./scripts/build_svm.sh

# 2. Train OPT_V2 (fastest)
./build_cuda/autoencoder_cuda_opt_v2 \
    cifar-10-binary/cifar-10-batches-bin \
    weights_opt_v2.bin \
    200 32 0.001

# 3. Extract features with Z-score scaling
./build_svm/extract_features_cuda \
    cifar-10-binary/cifar-10-batches-bin \
    weights_opt_v2.bin \
    train_scaled.libsvm \
    test_scaled.libsvm

# 4. Train SVM
thundersvm-train -s 0 -t 2 -c 1.0 -g 0.000122 \
    train_scaled.libsvm svm_model.txt

# 5. Predict
thundersvm-predict test_scaled.libsvm svm_model.txt predictions.txt

echo "Pipeline complete! Check accuracy above."
```

**Total time**: ~15-20 phÃºt

---

## ğŸ“Š EXPECTED RESULTS

### Loss convergence (OPT_V2, 200 epochs)

```
Epoch 1/200:   loss 0.228 | time: 1.2s
Epoch 10/200:  loss 0.087 | time: 1.1s
Epoch 50/200:  loss 0.043 | time: 1.1s
Epoch 100/200: loss 0.028 | time: 1.1s
Epoch 200/200: loss 0.019 | time: 1.1s
Total: ~4 min 30s
```

### Feature extraction output

```
=== CUDA Feature Extraction for SVM (With Z-Score Scaling) ===
PASS 1: Extracting train features + computing statistics...
  Completed 50000/50000
  Statistics computed (mean/std for 8192 dims)
PASS 2: Scaling and writing train features...
  Completed: 50000 samples
Extracting and scaling test features...
  Completed: 10000 samples
Total time: 429s

Scaling statistics:
  Samples:  50000
  Features: 8192
  Example mean[0]:   0.160194
  Example stddev[0]: 0.118832
```

### SVM training output

```
*
optimization finished, #iter = 12543
obj = -8234.567, rho = -0.123
nSV = 18234, nBSV = 15123
Total nSV = 18234
*
Accuracy = 62.34% (6234/10000)
```

---

## ğŸ¯ Káº¾T LUáº¬N

Pipeline Ä‘Ã£ Ä‘Æ°á»£c optimize qua 3 giai Ä‘oáº¡n:

1. **CUDA Basic** â†’ **OPT_V1**: ~30% faster (memory optimization)
2. **OPT_V1** â†’ **OPT_V2**: ~30% faster (speed optimization)
3. **Overall speedup**: **~10x faster than CPU**

Feature scaling (Z-score) cáº£i thiá»‡n SVM accuracy tá»« **46% â†’ 62%** (+16%).

Káº¿t há»£p cáº£ 2 optimizations Ä‘áº¡t Ä‘Æ°á»£c:
- âš¡ Training time: **4-5 phÃºt** (vs 45 phÃºt CPU)
- ğŸ“ˆ SVM accuracy: **~62%** vá»›i CIFAR-10
- ğŸ’¾ Memory efficient vá»›i 2-pass pipeline

---

**Author**: CSC14120 Final Project  
**Date**: December 2025  
**GPU**: NVIDIA RTX 3050 Laptop  
**CUDA**: 12.0
