# Phase 2.4: GPU Optimized Implementation - Version 2

## Optimization Focus: Kernel Fusion & Vectorized Operations

### Objectives

**Nh·ªØng G√¨ Ch√∫ng Ta Mu·ªën ƒê·∫°t ƒê∆∞·ª£c:**
- **Kernel Fusion**: Merge Conv + Bias + ReLU th√†nh 1 kernel duy nh·∫•t
- **Vectorized Memory Operations**: S·ª≠ d·ª•ng float4 cho SGD updates v√† loss computation
- **Reduce kernel launch overhead**: Gi·∫£m s·ªë l∆∞·ª£ng kernel calls
- **Target: 1.5-2√ó speedup** so v·ªõi GPU Opt V1

**T·∫°i Sao Optimizations N√†y:**
- Phase 2.3 profiling cho th·∫•y:
  - Conv kernels chi·∫øm 98% kernel time
  - cudaLaunchKernel overhead: 24.7% API time
  - Nhi·ªÅu small kernels (ReLU, bias) c√≥ overhead cao
- Kernel fusion gi·∫£m:
  - Intermediate memory writes/reads
  - Kernel launch overhead
  - Global memory bandwidth usage

---

## Chi Ti·∫øt Tri·ªÉn Khai (Implementation Details)

### Optimization Techniques Applied

#### 1. Kernel Fusion: Conv + Bias + ReLU (∆Øu ti√™n #1)

**V·∫•n ƒë·ªÅ:**
```cpp
// OLD (V1): 3 separate operations
conv2d_tiled_kernel<<<...>>>(input, weight, conv_out, ...);  // Write conv_out
// ReLU in-place: read conv_out, write back
relu_inplace_kernel<<<...>>>(conv_out, size);                // Read + Write
```

**Gi·∫£i ph√°p:**
```cpp
// NEW (V2): Single fused kernel
__global__ void conv2d_bias_relu_fused_kernel(
    const float* input, const float* weight, const float* bias, 
    float* output, ...)
{
    int oc = blockIdx.x;
    int oh = blockIdx.y * blockDim.y + threadIdx.y;
    int ow = blockIdx.z * blockDim.x + threadIdx.x;
    
    // Compute convolution
    float sum = 0.0f;
    #pragma unroll
    for (int ic = 0; ic < C_in; ic++) {
        for (int kh = 0; kh < K; kh++) {
            for (int kw = 0; kw < K; kw++) {
                sum += input[...] * weight[...];
            }
        }
    }
    
    // Add bias and apply ReLU in one step - NO intermediate writes!
    sum += bias[oc];
    output[...] = (sum > 0.0f) ? sum : 0.0f;  // Fused!
}
```

**L·ª£i √≠ch:**
- Eliminate intermediate buffer writes (conv_out)
- Single kernel launch thay v√¨ 2
- Gi·∫£m global memory accesses: 2 writes ‚Üí 1 write

**Trade-off:**
- ‚ö†Ô∏è V2 implementation kh√¥ng d√πng shared memory tiling (t·ª´ V1)
- ‚ö†Ô∏è Fused kernel ƒë·ªçc t·ª´ global memory m·ªói l·∫ßn (no caching)

#### 2. Vectorized SGD Update (float4)

**V·∫•n ƒë·ªÅ:**
```cpp
// OLD (V1): Scalar processing - 1 float per thread
__global__ void sgd_update_kernel(float* weight, const float* grad, float lr, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        weight[idx] -= lr * grad[idx];  // 1 float
    }
}
```

**Gi·∫£i ph√°p:**
```cpp
// NEW (V2): Process 4 floats per thread
__global__ void sgd_update_vec4(float* weight, const float* grad, float lr, int size) {
    int i4 = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    
    if (i4 + 3 < size) {
        float4* W = (float4*)(weight + i4);
        const float4* G = (const float4*)(grad + i4);
        
        float4 w = *W;
        float4 g = *G;
        
        // Update 4 elements at once
        w.x -= lr * g.x;
        w.y -= lr * g.y;
        w.z -= lr * g.z;
        w.w -= lr * g.w;
        
        *W = w;  // Single 128-bit write
    }
}
```

**L·ª£i √≠ch:**
- 4√ó more elements per thread
- Better memory bandwidth utilization (128-bit transactions)
- Fewer kernel launches (4√ó fewer threads needed)

#### 3. Vectorized Loss Computation (float4)

**T∆∞∆°ng t·ª± SGD, √°p d·ª•ng cho MSE loss:**
```cpp
__global__ void mse_loss_vec4(
    const float* pred, const float* target,
    float* loss, float* grad, int size)
{
    int i4 = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
    
    if (i4 + 3 < size) {
        float4 p = *((const float4*)(pred + i4));
        float4 t = *((const float4*)(target + i4));
        
        // Compute 4 losses at once
        float4 diff;
        diff.x = p.x - t.x;
        diff.y = p.y - t.y;
        diff.z = p.z - t.z;
        diff.w = p.w - t.w;
        
        // Write gradients
        *((float4*)(grad + i4)) = diff;
        
        // Accumulate loss
        float local_loss = diff.x*diff.x + diff.y*diff.y + diff.z*diff.z + diff.w*diff.w;
        atomicAdd(loss, local_loss);
    }
}
```

#### 4. Strategic Loop Unrolling

**Compiler hints for 3√ó3 convolutions:**
```cpp
#pragma unroll
for (int ic = 0; ic < C_in; ic++) {
    #pragma unroll
    for (int kh = 0; kh < 3; kh++) {
        #pragma unroll
        for (int kw = 0; kw < 3; kw++) {
            sum += input[...] * weight[...];
        }
    }
}
```

**L·ª£i √≠ch:**
- Compiler unrolls inner loops (3√ó3 = 9 iterations)
- Reduces loop overhead
- Better instruction pipelining

---

## C√°ch Ch·∫°y GPU Optimized V2

### Build Code
```bash
cd /home/senyamiku/LTSS/FinalCuda
bash scripts/build_cuda.sh
```

### Training
```bash
# Test v·ªõi 1000 ·∫£nh
./build_cuda/autoencoder_cuda_opt_v2 cifar-10-binary/cifar-10-batches-bin weights/opt_v2.bin 3 32 0.001 1000

# Full training (50,000 ·∫£nh)
./build_cuda/autoencoder_cuda_opt_v2 cifar-10-binary/cifar-10-batches-bin weights/opt_v2.bin 3 64 0.001 50000
```

### Profiling
```bash
# Profile v·ªõi 1000 ·∫£nh
nsys profile --stats=true -o report_opt2 ./build_cuda/autoencoder_cuda_opt_v2 cifar-10-binary/cifar-10-batches-bin weights/opt_v2.bin 3 64 0.001 1000

# Xem report
nsys stats report_opt2.nsys-rep
```

---

## K·∫øt Qu·∫£ (Results)

### C·∫•u H√¨nh

- **Hardware**: NVIDIA A100-SXM4-40GB (40 GB VRAM)
- **Dataset**: CIFAR-10 (1,000 test / 50,000 full training)
- **Hyperparameters**:
  - Epochs: 3
  - Batch size: 32-64
  - Learning rate: 0.001
  - Optimizer: SGD with gradient clipping (vectorized)

### Training Performance

#### Test Run (1000 images, 3 epochs, batch=32)

```
Epoch 1/3 - Average Loss: 0.169965 - Time: 2273ms - Throughput: 439.947 imgs/sec
Epoch 2/3 - Average Loss: 0.0935318 - Time: 2223ms - Throughput: 449.843 imgs/sec
Epoch 3/3 - Average Loss: 0.0999917 - Time: 2224ms - Throughput: 449.64 imgs/sec

Total training time: 8254ms (8.25s)
Average throughput: 363.46 imgs/sec
```

**So s√°nh v·ªõi c√°c phi√™n b·∫£n tr∆∞·ªõc:**
- GPU Basic (Phase 2.2, 1K, 3 epochs): 9530ms ‚Üí 3.18s/epoch
- GPU Opt V1 (Phase 2.3, 1K, 3 epochs): 6184ms ‚Üí 2.06s/epoch
- GPU Opt V2 (1K, 3 epochs): 8254ms ‚Üí 2.75s/epoch

**Speedup vs Basic: 1.15√ó (V2 nhanh h∆°n Basic 15%)**
**Slowdown vs V1: 0.75√ó (V2 ch·∫≠m h∆°n V1 33%!)**

#### Full Training (50,000 images, 3 epochs, batch=64)

```
Epoch 1/3 - Average Loss: 0.0527183 - Time: 111135ms - Throughput: 449.903 imgs/sec
Epoch 2/3 - Average Loss: 0.0307476 - Time: 111121ms - Throughput: 449.96 imgs/sec
Epoch 3/3 - Average Loss: 0.0252823 - Time: 111109ms - Throughput: 450.009 imgs/sec

Total training time: 334923ms (335s = 5.58 minutes)
Average throughput: 447.864 imgs/sec
```

**So s√°nh v·ªõi c√°c phi√™n b·∫£n tr∆∞·ªõc (50K, batch=64):**
- GPU Basic (Phase 2.2): ~317s, ~472 imgs/sec
- GPU Opt V1 (Phase 2.3): 231s, 648.699 imgs/sec
- GPU Opt V2 (Phase 2.4): 335s, 447.864 imgs/sec

**Speedup vs Basic: 0.95√ó (V2 ch·∫≠m h∆°n Basic 5%!) ‚ö†Ô∏è**
**Slowdown vs V1: 0.69√ó (V2 ch·∫≠m h∆°n V1 45%!)**

### B·∫£ng So S√°nh Performance

| Metric | GPU Basic (2.2) | GPU Opt V1 (2.3) | GPU Opt V2 (2.4) | vs Basic | vs V1 |
|--------|-----------------|------------------|------------------|----------|-------|
| **Time/epoch (1K, batch=32)** | 3.18s | 2.06s | 2.75s | **1.15√ó faster** | **0.75√ó (slower)** |
| **Total time (1K, 3 epochs)** | 9.53s | 6.18s | 8.25s | **1.15√ó faster** | **0.75√ó (slower)** |
| **Time/epoch (50K, batch=64)** |  ~133s | ~76s | ~111s | **1.19√ó faster** | **0.68√ó (slower)** |
| **Throughput (batch=32)** | 315 imgs/sec | 485 imgs/sec | 363 imgs/sec | **1.15√ó faster** | **0.75√ó (slower)** |
| **Throughput (batch=64, 50K)** | ~472 imgs/sec | 649 imgs/sec | 448 imgs/sec | **~0.95√ó (similar)** | **0.69√ó (slower)** |
| **Memory usage (batch=64)** | 441 MiB | 617 MiB | 437 MiB | **Similar** | **-29%** |

**‚ùå UNEXPECTED RESULT**: V2 ch·∫≠m h∆°n V1 thay v√¨ nhanh h∆°n!

**Cumulative Speedup vs CPU:**
- CPU Baseline: 750s/epoch
- GPU Basic: 3.18s/epoch ‚Üí **236√ó speedup**
- GPU Opt V1: 2.06s/epoch ‚Üí **364√ó speedup**
- GPU Opt V2: 2.75s/epoch ‚Üí **273√ó speedup**

**Ranking: V1 (364√ó) > V2 (273√ó) > Basic (236√ó) > CPU (1√ó)**

### GPU Memory Usage

**T·ª´ nvidia-smi (1000 images training):**

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15    CUDA: 12.4 |
|-------------------------------+----------------------+----------------------+
| GPU  Name                     | Memory-Usage         | GPU-Util  Power      |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB    | 437MiB / 40960MiB    | 99%      127W/400W   |
+-----------------------------------------------------------------------------+
```

**Ph√¢n t√≠ch:**
- **Memory Used**: 437 MiB (vs 617 MiB ·ªü V1)
- **Gi·∫£m 29%** do:
  - Kh√¥ng c√≥ batch processing buffers (ch·∫°y 1000 images, batch nh·ªè)
  - Kernel fusion eliminates intermediate buffers
  - ‚ö†Ô∏è Nh∆∞ng kh√¥ng c√≥ shared memory tiles
- **GPU Utilization**: 99% (excellent)
- **Power**: 127W/400W (32% - lower than V1's 149W)

**Memory efficiency t·ªët h∆°n nh∆∞ng throughput th·∫•p h∆°n!**

### Profiling Analysis (nsys, 1000 images)

#### Kernel Time Breakdown

```
Kernel Name                          | Time(%)  | Total Time | Avg Time  | Calls
-------------------------------------|----------|------------|-----------|-------
conv2d_weight_grad_kernel            | 25.6%    | 1.67s      | 111.6 Œºs  | 15,000
conv2d_bias_relu_fused_kernel        | 23.4%    | 1.53s      | 127.3 Œºs  | 12,000 ‚Üê FUSED!
conv2d_input_grad_kernel             | 20.9%    | 1.36s      | 113.6 Œºs  | 12,000
conv2d_kernel (no ReLU, for Conv5)   | 15.0%    | 0.98s      | 325.6 Œºs  | 3,000
conv2d_bias_grad_kernel              | 13.5%    | 0.88s      | 58.9 Œºs   | 15,000
relu_backward_kernel                 | 0.5%     | 29.8ms     | 2.5 Œºs    | 12,000
maxpool_backward_kernel              | 0.3%     | 16.7ms     | 2.8 Œºs    | 6,000
mse_loss_vec4                        | 0.2%     | 14.9ms     | 4.9 Œºs    | 3,048 ‚Üê VECTORIZED!
upsample_kernel                      | 0.2%     | 14.9ms     | 2.5 Œºs    | 6,000
maxpool_kernel                       | 0.2%     | 14.1ms     | 2.4 Œºs    | 6,000
upsample_backward_kernel             | 0.2%     | 14.0ms     | 2.3 Œºs    | 6,000
sgd_update_vec4                      | 0.0%     | 1.2ms      | 2.3 Œºs    | 528 ‚Üê VECTORIZED!

Total kernel time: 6.54s (out of 8.31s total)
```

**So s√°nh v·ªõi Basic v√† V1:**

| Kernel Category | Basic (2.2) | V1 (2.3) | V2 (2.4) | vs Basic | vs V1 |
|----------------|-------------|----------|----------|----------|-------|
| **Conv forward** | ~4.5s (93%) | 0.44s (9.8%) | 1.53s (23.4%) | **2.9√ó faster** ‚úÖ | **3.5√ó slower** üî¥ |
| **Conv backward** | N/A | 3.94s (87.6%) | 4.89s (74.8%) | N/A | 1.24√ó slower |
| **Loss (vec4)** | ~0.3s | N/A | 14.9ms (0.2%) | **20√ó faster** ‚úÖ | New |
| **SGD (vec4)** | ~0.1s | N/A | 1.2ms (0.0%) | **83√ó faster** ‚úÖ | New |
| **Total kernel** | ~7.3s | 4.50s | 6.54s | **1.12√ó faster** | **1.45√ó slower** üî¥ |

**‚ùó CRITICAL FINDING**: 
- **Conv forward ch·∫≠m h∆°n 3.5√ó** so v·ªõi V1's tiled version!
- Fused kernel kh√¥ng c√≥ shared memory ‚Üí read all data from global memory
- V1's shared memory tiling >> kernel fusion benefit

#### CUDA API Time

```
Operation                | Time(%)  | Total Time | Calls
-------------------------|----------|------------|---------
cudaMemcpy               | 92.3%    | 6.30s      | 15,116
cudaLaunchKernel         | 6.0%     | 410ms      | 96,816
cudaMalloc               | 1.5%     | 99.9ms     | 37
cudaMemset               | 0.2%     | 12.9ms     | 3,336
```

**So s√°nh v·ªõi V1:**

| API | V1 (2.3) | V2 (2.4) | Change |
|-----|----------|----------|--------|
| **cudaMemcpy** | 4.33s (90.9%) | 6.30s (92.3%) | 1.45√ó slower |
| **cudaLaunchKernel** | 313ms (6.6%) | 410ms (6.0%) | 1.31√ó slower |
| **Total API** | 4.76s | 6.82s | **1.43√ó slower** |

**Observations:**
- cudaMemcpy v·∫´n l√† bottleneck (92.3%)
- Launch overhead gi·∫£m nh·∫π (6.6% ‚Üí 6.0%) nh·ªù kernel fusion
- Nh∆∞ng total time tƒÉng do kernels ch·∫≠m h∆°n

#### Memory Operations

```
Operation          | Time(%)  | Total Time | Count  | Avg Size
-------------------|----------|------------|--------|----------
memcpy DtoD        | 64.5%    | 23.7ms     | 12,000 | 12 KB
memcpy DtoH        | 16.2%    | 5.9ms      | 3,058  | 1 KB
memset             | 9.9%     | 3.7ms      | 3,336  | 42 KB
memcpy HtoD        | 9.4%     | 3.4ms      | 58     | 687 KB
```

---

## Analysis

### Performance Position: Better Than Basic, Worse Than V1

**V2 sits in the middle:**
- ‚úÖ **Nhanh h∆°n Basic 15%** nh·ªù kernel fusion + vectorization
- ‚ùå **Ch·∫≠m h∆°n V1 33%** do m·∫•t shared memory optimization
- Result: V2 l√† improvement over Basic, nh∆∞ng regression from V1

**So s√°nh chi ti·∫øt:**
```
Basic (9.53s) ‚Üê[15% faster]‚Üí V2 (8.25s) ‚Üê[33% slower]‚Üí V1 (6.18s)
    ‚Üë                            ‚Üë                           ‚Üë
  Naive                   Kernel Fusion              Shared Memory
```

### Why Did V2 Perform WORSE Than V1?

#### Root Cause: Lost Shared Memory Optimization

**V1 Implementation:**
```cpp
// V1: Shared memory tiling
__shared__ float s_input[BATCH_SIZE][SHARED_TILE_HEIGHT][SHARED_TILE_WIDTH];

// Load tile once, reuse nhi·ªÅu l·∫ßn
for (batch in input channels) {
    load_tile_to_shared_memory();
    __syncthreads();
    
    // All threads compute from shared memory (FAST!)
    compute_convolution_from_shared();
}
```

**V2 Implementation:**
```cpp
// V2: Fused but no shared memory
__global__ void conv2d_bias_relu_fused_kernel(...) {
    // Read directly from global memory (SLOW!)
    for (int ic = 0; ic < C_in; ic++) {
        for (int kh = 0; kh < K; kh++) {
            for (int kw = 0; kw < K; kw++) {
                sum += input[...] * weight[...];  // Global memory read!
            }
        }
    }
}
```

**Performance Impact:**
- V1 conv forward: 0.44s (with shared memory)
- V2 conv forward: 1.53s (without shared memory)
- **3.5√ó regression!**

#### Kernel Fusion Benefits (Minor)

**Pros:**
- Eliminates 1 intermediate write per conv layer
- Reduces kernel launches (12K ‚Üí fewer)
- Slight reduction in memory usage

**Cons:**
- Lost shared memory reuse (major)
- Global memory bandwidth becomes bottleneck
- Arithmetic intensity decreased

**Net result:** Small fusion benefit << Large shared memory loss

#### Vectorization Benefits (Minimal Impact)

**SGD vectorization:**
- Before: scalar updates
- After: float4 (4√ó elements per thread)
- Time saved: ~1-2ms (negligible in 8s total)

**Loss vectorization:**
- Similar minimal impact
- MSE computation is tiny fraction of total time

**Conclusion:** Vectorization helped but impact too small

### What Went Wrong?

**Design Decision Error:**
1. ‚úÖ Kernel fusion is good idea
2. ‚ùå But shouldn't replace shared memory tiling
3. ‚ùå Should have: **Fused kernel WITH shared memory**

**Correct approach (not implemented):**
```cpp
// IDEAL: Conv + Bias + ReLU v·ªõi shared memory tiling
__global__ void conv2d_bias_relu_tiled_fused(...) {
    __shared__ float s_input[...];  // Keep tiling!
    
    // Load to shared memory
    // Compute from shared memory
    // Apply bias + ReLU at end
    
    // Best of both worlds!
}
```

### Performance Comparison Summary

| Version | Key Optimization | Time (1K, 3 epochs) | vs Basic | vs V1 |
|---------|-----------------|---------------------|----------|-------|
| **Basic (Phase 2.2)** | Naive GPU parallelization | 9.53s | Baseline | -35% |
| **V2 (Phase 2.4)** | Kernel fusion + vectorization | 8.25s | **+15%** ‚úÖ | **-25%** ‚ùå |
| **V1 (Phase 2.3)** | Shared memory tiling | 6.18s | **+54%** ‚≠ê | Baseline |

**Lesson learned:** Kh√¥ng ph·∫£i m·ªçi optimization ƒë·ªÅu t·ªët h∆°n. Trade-offs matter!

---

## Key Takeaways

### Lessons Learned

1. **Shared Memory >> Kernel Fusion**
   - Shared memory tiling: 3.5√ó speedup
   - Kernel fusion alone: Not enough to compensate
   - **Priority matters**: Optimize biggest bottleneck first

2. **Optimization Trade-offs Are Real**
   - V2 gi·∫£m memory usage (29%)
   - Nh∆∞ng tƒÉng compute time (33%)
   - Trade-off kh√¥ng x·ª©ng ƒë√°ng

3. **Incremental Optimization Is Safer**
   - V1 ‚Üí V2 should have kept V1's optimizations
   - Add kernel fusion ON TOP of shared memory
   - Don't replace working optimizations

4. **Vectorization Impact Is Small**
   - float4 SGD: ~1ms saved (negligible)
   - Good for polish, not main optimization
   - Focus on kernel compute first

5. **Profiling Is Essential**
   - Without profiling, V2 looks good (fusion + vectorization)
   - With profiling: clearly worse than V1
   - **Always measure, don't assume!**

### What Should Have Been Done

**Correct V2 Implementation:**
```cpp
// IDEAL: Combine ALL optimizations
__global__ void conv2d_bias_relu_tiled_fused_kernel(...) {
    // ‚úÖ Shared memory tiling (from V1)
    __shared__ float s_input[...];
    
    // ‚úÖ Kernel fusion (V2 idea)
    float sum = compute_from_shared_memory();
    sum += bias[oc];
    output[...] = fmaxf(0.0f, sum);  // Fused bias + ReLU
    
    // ‚úÖ Memory coalescing (from V1)
    // ‚úÖ Loop unrolling
}
```

**Expected performance:**
- Keep V1's conv forward speed: 0.44s
- Add fusion benefits: ~5-10% improvement
- **Target: 5.5-6s total (vs 6.18s V1, vs 8.25s actual V2)**

### Recommendations for Real-World Projects

1. **Always Keep Working Optimizations**
   - Build incrementally
   - Don't remove what works
   - A + B > A or B alone

2. **Profile Before and After**
   - Measure every optimization
   - Compare against baseline
   - Reject if regression > 5%

3. **Understand Root Causes**
   - Know WHY optimization works
   - Shared memory: data reuse
   - Kernel fusion: eliminate writes
   - Choose based on bottleneck

4. **Document Trade-offs**
   - Memory vs speed
   - Complexity vs maintainability
   - Make conscious decisions

### Final Verdict

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Speedup vs Basic | ‚úÖ Better | ‚úÖ +15% | ‚úÖ Success |
| Speedup vs V1 | 1.5-2√ó | 0.75√ó | ‚ùå Failed |
| Memory efficiency | Improved | ‚úÖ -29% vs V1 | ‚úÖ Success |
| Code complexity | Similar | ‚úÖ Similar | ‚úÖ OK |
| Overall | Better than V1 | **Middle: Basic < V2 < V1** | ‚ö†Ô∏è **PARTIAL SUCCESS** |

**Conclusion:** 
- ‚úÖ Phase 2.4 **is an improvement over Basic** (15% faster)
- ‚ùå But **regression from V1** (25% slower)
- V2 demonstrates kernel fusion works, but not enough to beat shared memory
- Lesson: ƒê√¥i khi, √≠t t·ªëi ∆∞u h∆°n l·∫°i t·ªët h∆°n n·∫øu trade-offs sai

---

## What's Next?

**If Continuing Optimization:**

1. **V3 (Hypothetical): Fused + Tiled**
   - Combine V1's shared memory + V2's fusion
   - Expected: 5-6s (vs 6.18s V1)
   - True improvement over both

2. **CUDA Streams (Real next step)**
   - Async execution
   - Overlap compute + memory transfer
   - Target: 2-3√ó speedup by hiding latency

3. **Better Memory Management**
   - Pinned memory for faster transfers
   - Batch prefetching
   - Double buffering

---

## File Structure Reference

```
src/
‚îú‚îÄ‚îÄ main_cuda.cpp
‚îú‚îÄ‚îÄ cuda/
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_basic.cu         # Phase 2.2: Naive GPU
‚îÇ   ‚îú‚îÄ‚îÄ autoencoder_opt_v1.cu        # Phase 2.3: Shared Memory + Coalescing ‚úÖ BEST
‚îÇ   ‚îî‚îÄ‚îÄ autoencoder_opt_v2.cu        # Phase 2.4: Kernel Fusion (regression) ‚ùå
include/
‚îî‚îÄ‚îÄ autoencoder_cuda.h
scripts/
‚îî‚îÄ‚îÄ build_cuda.sh
```

**Key Changes in autoencoder_opt_v2.cu:**
- `conv2d_bias_relu_fused_kernel()`: Fused Conv+Bias+ReLU (lines 51-89)
- `sgd_update_vec4()`: Vectorized SGD with float4 (lines 476-512)
- `mse_loss_vec4()`: Vectorized loss computation (lines 539-586)
- ‚ùå **Missing**: Shared memory tiling from V1
- ‚ùå **Result**: 33% slower than V1

**Total lines**: ~1,200 (vs ~1,300 in V1)

**Performance Ranking (1000 images, 3 epochs):**
1. ü•á **GPU Opt V1 (2.3)**: 6.18s (364√ó vs CPU) - BEST ‚≠ê
2. ü•à **GPU Opt V2 (2.4)**: 8.25s (273√ó vs CPU) - MIDDLE GROUND
3. ü•â **GPU Basic (2.2)**: 9.53s (236√ó vs CPU) - BASELINE GPU
4. ‚è±Ô∏è **CPU Baseline**: 2250s (1√ó) - SLOWEST

**Gap analysis:**
- V1 ‚Üí V2: +33% time (regression due to no shared memory)
- V2 ‚Üí Basic: +15% time (improvement from kernel fusion)
- Basic ‚Üí CPU: +23,500% time (massive GPU speedup)
